{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T22:17:19.603440Z",
     "start_time": "2019-02-12T22:17:19.523397Z"
    }
   },
   "source": [
    "<h1><center> Facial Emotion Recognition </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imutils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mndimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m zoom\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distance\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimutils\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ndimage\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'imutils'"
     ]
    }
   ],
   "source": [
    "### General imports ###\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from time import sleep\n",
    "import re\n",
    "import os\n",
    "import argparse\n",
    "from collections import OrderedDict\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "### Image processing ###\n",
    "from scipy.ndimage import zoom\n",
    "from scipy.spatial import distance\n",
    "import imutils\n",
    "from scipy import ndimage\n",
    "import cv2\n",
    "import dlib\n",
    "from __future__ import division\n",
    "from imutils import face_utils\n",
    "\n",
    "### CNN models ###\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, SeparableConv2D\n",
    "from keras.utils import np_utils\n",
    "from keras.regularizers import l2#, activity_l2\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import models\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.layers import Input, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "### Build SVM models ###\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm\n",
    "\n",
    "### Same trained models ###\n",
    "import h5py\n",
    "from keras.models import model_from_json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Import datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T11:32:27.251687Z",
     "start_time": "2019-03-05T11:32:27.246427Z"
    }
   },
   "outputs": [],
   "source": [
    "path = '/Users/maelfabien/filrouge_pole_emploi/Video/'\n",
    "local_path = '/Users/maelfabien/Desktop/LocalDB/Videos/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T11:32:45.974591Z",
     "start_time": "2019-03-05T11:32:27.254591Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/maelfabien/Desktop/LocalDB/Videos/fer2013.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m pd\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mmode\u001b[38;5;241m.\u001b[39mchained_assignment \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# default='warn'  #to suppress SettingWithCopyWarning\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#Reading the dataset\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(local_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfer2013.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#Obtaining train data where usage is \"Training\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m train \u001b[38;5;241m=\u001b[39m dataset[dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1662\u001b[0m     f,\n\u001b[1;32m   1663\u001b[0m     mode,\n\u001b[1;32m   1664\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1665\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1666\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1667\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1668\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1669\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1670\u001b[0m )\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    862\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    863\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/maelfabien/Desktop/LocalDB/Videos/fer2013.csv'"
     ]
    }
   ],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'  #to suppress SettingWithCopyWarning\n",
    "\n",
    "#Reading the dataset\n",
    "dataset = pd.read_csv(local_path + 'fer2013.csv')\n",
    "\n",
    "#Obtaining train data where usage is \"Training\"\n",
    "train = dataset[dataset[\"Usage\"] == \"Training\"]\n",
    "\n",
    "#Obtaining test data where usage is \"PublicTest\"\n",
    "test = dataset[dataset[\"Usage\"] == \"PublicTest\"]\n",
    "\n",
    "#Converting \" \" separated pixel values to list\n",
    "train['pixels'] = train['pixels'].apply(lambda image_px : np.fromstring(image_px, sep = ' '))\n",
    "test['pixels'] = test['pixels'].apply(lambda image_px : np.fromstring(image_px, sep = ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T11:32:46.001801Z",
     "start_time": "2019-03-05T11:32:45.978732Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T11:32:46.020748Z",
     "start_time": "2019-03-05T11:32:46.005453Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset[dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memotion\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataset[dataset['emotion'] == 1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T11:32:46.635647Z",
     "start_time": "2019-03-05T11:32:46.024909Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mhist(dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memotion\u001b[39m\u001b[38;5;124m'\u001b[39m], bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDistribution of the number of images per emotion\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.hist(dataset['emotion'], bins=30)\n",
    "plt.title(\"Distribution of the number of images per emotion\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T11:32:46.644837Z",
     "start_time": "2019-03-05T11:32:46.638925Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T11:32:46.654817Z",
     "start_time": "2019-03-05T11:32:46.648983Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Create the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T11:32:49.928791Z",
     "start_time": "2019-03-05T11:32:49.925843Z"
    }
   },
   "outputs": [],
   "source": [
    "shape_x = 48\n",
    "shape_y = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T11:32:51.226415Z",
     "start_time": "2019-03-05T11:32:50.281701Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_train \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m      2\u001b[0m y_train \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m      3\u001b[0m X_test \u001b[38;5;241m=\u001b[39m test\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "X_train = train.iloc[:, 1].values\n",
    "y_train = train.iloc[:, 0].values\n",
    "X_test = test.iloc[:, 1].values\n",
    "y_test = test.iloc[:, 0].values\n",
    "\n",
    "#np.vstack stack arrays in sequence vertically (picking element row wise)\n",
    "X_train = np.vstack(X_train)\n",
    "X_test = np.vstack(X_test)\n",
    "\n",
    "#Reshape X_train, y_train,X_test,y_test in desired formats\n",
    "X_train = np.reshape(X_train, (X_train.shape[0],48,48,1))\n",
    "y_train = np.reshape(y_train, (y_train.shape[0],1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0],48,48,1))\n",
    "y_test = np.reshape(y_test, (y_test.shape[0],1))\n",
    "\n",
    "print(\"Shape of X_train and y_train is \" + str(X_train.shape) +\" and \" + str(y_train.shape) +\" respectively.\")\n",
    "print(\"Shape of X_test and y_test is \" + str(X_test.shape) +\" and \" + str(y_test.shape) +\" respectively.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T11:32:53.129990Z",
     "start_time": "2019-03-05T11:32:52.617079Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Change to float datatype\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_data \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m test_data \u001b[38;5;241m=\u001b[39m X_test\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Scale the data to lie between 0 to 1\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Change to float datatype\n",
    "train_data = X_train.astype('float32')\n",
    "test_data = X_test.astype('float32')\n",
    "\n",
    "# Scale the data to lie between 0 to 1\n",
    "train_data /= 255\n",
    "test_data /= 255\n",
    "\n",
    "# Change the labels from integer to categorical data\n",
    "train_labels_one_hot = to_categorical(y_train)\n",
    "test_labels_one_hot = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Define the number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T11:32:53.658035Z",
     "start_time": "2019-03-05T11:32:53.650012Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Find the unique numbers from the train labels\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y_train)\n\u001b[1;32m      3\u001b[0m nClasses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(classes)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal number of outputs : \u001b[39m\u001b[38;5;124m'\u001b[39m, nClasses)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Find the unique numbers from the train labels\n",
    "classes = np.unique(y_train)\n",
    "nClasses = len(classes)\n",
    "print('Total number of outputs : ', nClasses)\n",
    "print('Output classes : ', classes)\n",
    "\n",
    "# Find the shape of input images and create the variable input_shape\n",
    "nRows,nCols,nDims = X_train.shape[1:]\n",
    "input_shape = (nRows, nCols, nDims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T11:32:55.787579Z",
     "start_time": "2019-03-05T11:32:55.780343Z"
    }
   },
   "outputs": [],
   "source": [
    "#Defining labels \n",
    "def get_label(argument):\n",
    "    labels = {0:'Angry', 1:'Disgust', 2:'Fear', 3:'Happy', 4:'Sad' , 5:'Surprise', 6:'Neutral'}\n",
    "    return(labels.get(argument, \"Invalid emotion\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T13:22:31.497497Z",
     "start_time": "2019-03-05T13:22:30.972645Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Display the first image in training data\u001b[39;00m\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m121\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(np\u001b[38;5;241m.\u001b[39msqueeze(X_train[\u001b[38;5;241m25\u001b[39m,:,:], axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m), cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGround Truth : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(get_label(\u001b[38;5;28mint\u001b[39m(y_train[\u001b[38;5;241m0\u001b[39m]))))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Display the first image in testing data\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=[10,5])\n",
    "\n",
    "# Display the first image in training data\n",
    "plt.subplot(121)\n",
    "plt.imshow(np.squeeze(X_train[25,:,:], axis = 2), cmap='gray')\n",
    "plt.title(\"Ground Truth : {}\".format(get_label(int(y_train[0]))))\n",
    "\n",
    "# Display the first image in testing data\n",
    "plt.subplot(122)\n",
    "plt.imshow(np.squeeze(X_test[26,:,:], axis = 2), cmap='gray')\n",
    "plt.title(\"Ground Truth : {}\".format(get_label(int(y_test[1500]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI. Detect Faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we need to detect the faces inside an image. This will allow us to :\n",
    "- focus on the region of the face\n",
    "- stop the prediction if no face is recognized.\n",
    "\n",
    "To do so, we use OpenCV faceCascade classifier. Object Detection using Haar feature-based cascade classifiers is an effective object detection method proposed by Paul Viola and Michael Jones in their paper, \"Rapid Object Detection using a Boosted Cascade of Simple Features\" in 2001. It is a machine learning based approach where a cascade function is trained from a lot of positive and negative images. It is then used to detect objects in other images.\n",
    "\n",
    "The term \"Cascade\" comes from the fact that when a window is explored and no face edge is identified, the region is left apart and we move on to the next one using Adaboost classifier. This makes the overall process very efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T11:32:59.685811Z",
     "start_time": "2019-03-05T11:32:59.672784Z"
    }
   },
   "outputs": [],
   "source": [
    "def detect_face(frame):\n",
    "    \n",
    "    #Cascade classifier pre-trained model\n",
    "    cascPath = '/usr/local/lib/python3.7/site-packages/cv2/data/haarcascade_frontalface_default.xml'\n",
    "    faceCascade = cv2.CascadeClassifier(cascPath)\n",
    "    \n",
    "    #BGR -> Gray conversion\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    #Cascade MultiScale classifier\n",
    "    detected_faces = faceCascade.detectMultiScale(gray,scaleFactor=1.1,minNeighbors=6,\n",
    "                                                  minSize=(shape_x, shape_y),\n",
    "                                                  flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "    coord = []\n",
    "    \n",
    "    for x, y, w, h in detected_faces :\n",
    "        if w > 100 :\n",
    "            sub_img=frame[y:y+h,x:x+w]\n",
    "            #cv2.rectangle(frame,(x,y),(x+w,y+h),(0, 255,255),1)\n",
    "            coord.append([x,y,w,h])\n",
    "    \n",
    "    return gray, detected_faces, coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T11:33:00.455025Z",
     "start_time": "2019-03-05T11:33:00.436863Z"
    }
   },
   "outputs": [],
   "source": [
    "#Extraire les features faciales\n",
    "def extract_face_features(faces, offset_coefficients=(0.075, 0.05)):\n",
    "    gray = faces[0]\n",
    "    detected_face = faces[1]\n",
    "    \n",
    "    new_face = []\n",
    "    \n",
    "    for det in detected_face :\n",
    "        #Region dans laquelle la face est détectée\n",
    "        x, y, w, h = det\n",
    "        #X et y correspondent à la conversion en gris par gray, et w, h correspondent à la hauteur/largeur\n",
    "    \n",
    "        #Offset coefficient, np.floor takes the lowest integer (delete border of the image)\n",
    "        horizontal_offset = np.int(np.floor(offset_coefficients[0] * w))\n",
    "        vertical_offset = np.int(np.floor(offset_coefficients[1] * h))\n",
    "\n",
    "        #gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        #gray transforme l'image\n",
    "        extracted_face = gray[y+vertical_offset:y+h, x+horizontal_offset:x-horizontal_offset+w]\n",
    "    \n",
    "        #Zoom sur la face extraite\n",
    "        new_extracted_face = zoom(extracted_face, (shape_x / extracted_face.shape[0],shape_y / extracted_face.shape[1]))\n",
    "        #cast type float\n",
    "        new_extracted_face = new_extracted_face.astype(np.float32)\n",
    "        #scale\n",
    "        new_extracted_face /= float(new_extracted_face.max())\n",
    "        #print(new_extracted_face)\n",
    "    \n",
    "        new_face.append(new_extracted_face)\n",
    "    \n",
    "    return new_face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial picture :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T11:33:02.284491Z",
     "start_time": "2019-03-05T11:33:01.825989Z"
    }
   },
   "outputs": [],
   "source": [
    "trump = '/Users/maelfabien/filrouge_pole_emploi/Video/test_samples/trump.jpg'\n",
    "trump_face = cv2.imread(trump, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(trump_face)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracted face :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T11:33:03.795625Z",
     "start_time": "2019-03-05T11:33:03.443966Z"
    }
   },
   "outputs": [],
   "source": [
    "face = extract_face_features(detect_face(trump_face))[0]\n",
    "plt.imshow(face)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VII. Deep Learning Model architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A first simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T08:44:14.026915Z",
     "start_time": "2019-02-12T08:44:13.999984Z"
    }
   },
   "outputs": [],
   "source": [
    "def createModel():\n",
    "    \n",
    "    #Model Initialization\n",
    "    model = Sequential() \n",
    "    \n",
    "    #Adding Input Layer\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=input_shape))\n",
    "    \n",
    "    #Adding more layers\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    " \n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    " \n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "  \n",
    "    #Flattening\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    #Adding fully connected layer\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    \n",
    "    model.add(Dropout(0.6))\n",
    "    \n",
    "    #Adding Output Layer\n",
    "    model.add(Dense(nClasses, activation='softmax'))\n",
    "     \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prevent Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T08:44:14.295268Z",
     "start_time": "2019-02-12T08:44:14.276272Z"
    }
   },
   "outputs": [],
   "source": [
    "def createModel2():\n",
    "    \n",
    "    #Model Initialization\n",
    "    model = Sequential() \n",
    "    \n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    " \n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
    "    \n",
    "    #Flattening\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    #Adding fully connected layer\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    \n",
    "    #Adding Output Layer\n",
    "    model.add(Dense(nClasses, activation='softmax'))\n",
    "     \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Go Deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T13:04:41.083473Z",
     "start_time": "2019-02-19T13:04:41.046749Z"
    }
   },
   "outputs": [],
   "source": [
    "def createModel3():\n",
    "    \n",
    "    #Model Initialization\n",
    "    model = Sequential() \n",
    "    \n",
    "    model.add(Conv2D(20, (3, 3), padding='same', activation='relu', input_shape=input_shape))\n",
    "    model.add(Conv2D(30, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Conv2D(40, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(50, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Conv2D(60, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(70, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Conv2D(80, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(90, (3, 3), padding='same', activation='relu'))\n",
    "    \n",
    "    #Flattening\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    #Adding fully connected layer\n",
    "    model.add(Dense(1000, activation='relu'))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    \n",
    "    #Adding Output Layer\n",
    "    model.add(Dense(nClasses, activation='softmax'))\n",
    "     \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T13:04:46.905112Z",
     "start_time": "2019-02-19T13:04:46.318877Z"
    }
   },
   "outputs": [],
   "source": [
    "model = createModel3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T13:04:47.669662Z",
     "start_time": "2019-02-19T13:04:47.659239Z"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And visualize the model architecture :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T13:04:50.165553Z",
     "start_time": "2019-02-19T13:04:50.017263Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_model(model, to_file='model_images/model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='model_images/model_plot.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIII. Visualize layers and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T13:53:30.927146Z",
     "start_time": "2019-02-19T13:53:30.922535Z"
    }
   },
   "outputs": [],
   "source": [
    "layer_outputs = [layer.output for layer in model.layers[:12]] \n",
    "# Extracts the outputs of the top 12 layers\n",
    "activation_model = models.Model(inputs=model.input, outputs=layer_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T13:53:31.484045Z",
     "start_time": "2019-02-19T13:53:31.479980Z"
    }
   },
   "outputs": [],
   "source": [
    "layer_names = []\n",
    "for layer in model.layers[:12]:\n",
    "    layer_names.append(layer.name) # Names of the layers, so you can have them as part of your plot\n",
    "    \n",
    "images_per_row = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T13:53:32.373532Z",
     "start_time": "2019-02-19T13:53:32.094461Z"
    }
   },
   "outputs": [],
   "source": [
    "trump = '/Users/maelfabien/filrouge_pole_emploi/Video/test_samples/trump.jpg'\n",
    "trump_face = cv2.imread(trump)\n",
    "face = extract_face_features(detect_face(trump_face))[0]\n",
    "\n",
    "to_predict = np.reshape(face.flatten(), (1,48,48,1))\n",
    "res = model.predict(to_predict)\n",
    "activations = activation_model.predict(to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T13:53:33.483811Z",
     "start_time": "2019-02-19T13:53:32.879713Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.title(\"Original Face\")\n",
    "plt.imshow(trump_face)\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.title(\"Extracted Face\")\n",
    "plt.imshow(face)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T13:53:37.876139Z",
     "start_time": "2019-02-19T13:53:34.566439Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for layer_name, layer_activation in zip(layer_names, activations): # Displays the feature maps\n",
    "    n_features = layer_activation.shape[-1] # Number of features in the feature map\n",
    "    size = layer_activation.shape[1] #The feature map has shape (1, size, size, n_features).\n",
    "    n_cols = n_features // images_per_row # Tiles the activation channels in this matrix\n",
    "    display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
    "    for col in range(n_cols): # Tiles each filter into a big horizontal grid\n",
    "        for row in range(images_per_row):\n",
    "            channel_image = layer_activation[0,:, :,col * images_per_row + row]\n",
    "            channel_image -= channel_image.mean() # Post-processes the feature to make it visually palatable\n",
    "            channel_image /= channel_image.std()\n",
    "            channel_image *= 64\n",
    "            channel_image += 128\n",
    "            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "            display_grid[col * size : (col + 1) * size, # Displays the grid\n",
    "                         row * size : (row + 1) * size] = channel_image\n",
    "    scale = 1. / size\n",
    "    plt.figure(figsize=(scale * display_grid.shape[1],\n",
    "                        scale * display_grid.shape[0]))\n",
    "    plt.title(layer_name)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(display_grid, aspect='auto', cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IX. Create and train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prevent overfitting, we can do data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T13:53:44.798652Z",
     "start_time": "2019-02-19T13:53:44.793239Z"
    }
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        zoom_range=0.2,          # randomly zoom into images\n",
    "        rotation_range=10,       # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,   # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,    # randomly flip images\n",
    "        vertical_flip=False)     # randomly flip images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T13:53:52.370011Z",
     "start_time": "2019-02-19T13:53:45.675780Z"
    }
   },
   "outputs": [],
   "source": [
    "#Creating 2nd model and training(fitting)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    " \n",
    "batch_size = 256\n",
    "epochs = 100\n",
    "\n",
    "# Fit the model on the batches generated by datagen.flow().\n",
    "history = model.fit_generator(\n",
    "    datagen.flow(train_data, train_labels_one_hot, batch_size=batch_size),\n",
    "    steps_per_epoch=int(np.ceil(train_data.shape[0] / float(batch_size))),\n",
    "    epochs = epochs, \n",
    "    validation_data=(test_data, test_labels_one_hot)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T22:19:44.845199Z",
     "start_time": "2019-02-12T22:19:43.830788Z"
    }
   },
   "outputs": [],
   "source": [
    "#Plotting accuracy and loss curves for 2nd model\n",
    "\n",
    "# Loss Curves\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(history.history['loss'],'r',linewidth=2.0)\n",
    "plt.plot(history.history['val_loss'],'b',linewidth=2.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.title('Loss Curves',fontsize=16)\n",
    " \n",
    "# Accuracy Curves\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(history.history['acc'],'r',linewidth=2.0)\n",
    "plt.plot(history.history['val_acc'],'b',linewidth=2.0)\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Accuracy',fontsize=16)\n",
    "plt.title('Accuracy Curves',fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XI. Save and re-open the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T13:11:45.831737Z",
     "start_time": "2019-02-19T13:11:45.500516Z"
    }
   },
   "outputs": [],
   "source": [
    "#save the model weights\n",
    "json_string = model.to_json()\n",
    "model.save_weights(local_path + 'savedmodels/model_3.h5')\n",
    "open(local_path + 'savedmodels/model_3.json', 'w').write(json_string)\n",
    "#model.save_weights(local_path + 'savedmodels/Emotion_Face_Detection_Model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T14:37:58.450090Z",
     "start_time": "2019-02-25T14:37:57.094325Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(local_path + 'savedmodels/model_3.json','r') as f:\n",
    "    json = f.read()\n",
    "model = model_from_json(json)\n",
    "\n",
    "model.load_weights(local_path + 'savedmodels/model_3.h5')\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XII. Making a prediction on an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T13:53:57.060253Z",
     "start_time": "2019-02-19T13:53:57.050514Z"
    }
   },
   "outputs": [],
   "source": [
    "hanks = '/Users/maelfabien/filrouge_pole_emploi/Video/test_samples/hanks_vs.jpg'\n",
    "hanks_face = cv2.imread(hanks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T13:53:58.056401Z",
     "start_time": "2019-02-19T13:53:57.614966Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(hanks_face, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T13:53:58.970238Z",
     "start_time": "2019-02-19T13:53:58.163867Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "plt.imshow(detect_face(hanks_face)[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T13:53:59.738856Z",
     "start_time": "2019-02-19T13:53:58.973905Z"
    }
   },
   "outputs": [],
   "source": [
    "for face in extract_face_features(detect_face(hanks_face)) :\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(face)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T13:54:00.013546Z",
     "start_time": "2019-02-19T13:53:59.741790Z"
    }
   },
   "outputs": [],
   "source": [
    "for face in extract_face_features(detect_face(hanks_face)) :\n",
    "    to_predict = np.reshape(face.flatten(), (1,48,48,1))\n",
    "    res = model.predict(to_predict)\n",
    "    result_num = np.argmax(res)\n",
    "    print(result_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T10:03:47.164468Z",
     "start_time": "2018-12-27T10:03:47.156886Z"
    }
   },
   "source": [
    "This corresponds to the Happy Labels which is a good prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XIII. Making live predictions from Webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T13:54:27.796947Z",
     "start_time": "2019-02-19T13:54:08.969256Z"
    }
   },
   "outputs": [],
   "source": [
    "#Lancer la capture video\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "emotion = []\n",
    "\n",
    "xs = []\n",
    "ys = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    # sleep(0.8)\n",
    "    ret, frame = video_capture.read()\n",
    "    \n",
    "    face_index = 0\n",
    "    gray, detected_faces, coord = detect_face(frame)\n",
    "\n",
    "    try :\n",
    "        for face in detected_faces :\n",
    "            face = extract_face_features(gray, face)\n",
    "            face = np.reshape(face.flatten(), (1,48,48,1))\n",
    "            x,y,w,h = detect_face(frame)[2][face_index]\n",
    "        \n",
    "            #if w > 200 : \n",
    "            #Dessiner rectangle autour de la tête\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "            \n",
    "            # predict smile\n",
    "            prediction = model.predict(face)\n",
    "            prediction_result = np.argmax(prediction)\n",
    "       \n",
    "            cv2.putText(frame, \"Angry : \" + str(round(prediction[0][0],3)),(10,30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 0)\n",
    "            cv2.putText(frame, \"Disgust : \" + str(round(prediction[0][1],3)),(10,50), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 0)\n",
    "            cv2.putText(frame, \"Fear : \" + str(round(prediction[0][2],3)),(10,70), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 1)\n",
    "            cv2.putText(frame, \"Happy : \" + str(round(prediction[0][3],3)),(10,90), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 1)\n",
    "            cv2.putText(frame, \"Sad : \" + str(round(prediction[0][4],3)),(10,110), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 1)\n",
    "            cv2.putText(frame, \"Surprise : \" + str(round(prediction[0][5],3)),(10,130), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 1)\n",
    "            cv2.putText(frame, \"Neutral : \" + str(round(prediction[0][6],3)),(10,150), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 1)\n",
    "        \n",
    "            # draw extracted face in the top right corner\n",
    "            #frame[face_index * shape_x: (face_index + 1) * shape_x, -1 * shape_y - 1:-1, :] = cv2.cvtColor(extracted_face * 255, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "            # annotate main image with a label\n",
    "            if prediction_result == 0 :\n",
    "                cv2.putText(frame, \"Angry\",(x,y), cv2.FONT_HERSHEY_SIMPLEX, 2, 155, 10)\n",
    "            elif prediction_result == 1 :\n",
    "                cv2.putText(frame, \"Disgust\",(x,y), cv2.FONT_HERSHEY_SIMPLEX, 2, 155, 10)\n",
    "            elif prediction_result == 2 :\n",
    "                cv2.putText(frame, \"Fear\",(x,y), cv2.FONT_HERSHEY_SIMPLEX, 2, 155, 10)\n",
    "            elif prediction_result == 3 :\n",
    "                cv2.putText(frame, \"Happy\",(x,y), cv2.FONT_HERSHEY_SIMPLEX, 2, 155, 10)\n",
    "            elif prediction_result == 4 :\n",
    "                cv2.putText(frame, \"Sad\",(x,y), cv2.FONT_HERSHEY_SIMPLEX, 2, 155, 10)\n",
    "            elif prediction_result == 5 :\n",
    "                cv2.putText(frame, \"Surprise\",(x,y), cv2.FONT_HERSHEY_SIMPLEX, 2, 155, 10)\n",
    "            else :\n",
    "                cv2.putText(frame, \"Neutral\",(x,y), cv2.FONT_HERSHEY_SIMPLEX, 2, 155, 10)\n",
    "           \n",
    "        #emotion.append(prediction_result)\n",
    "        #xs.append(i)\n",
    "        #ys.append(prediction_result)\n",
    "        \n",
    "        #line1, = ax.plot(i, prediction_result, 'b-') \n",
    "       # line1.set_ydata(prediction_result)\n",
    "       # fig.canvas.draw()\n",
    "            \n",
    "        #print(prediction_result)\n",
    "               \n",
    "            face_index += 1\n",
    "        i = i + 1\n",
    "    except :\n",
    "        continue \n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "# When everything is done, release the capture\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "#%matplotlib qt\n",
    "#plt.bar(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T11:11:01.475946Z",
     "start_time": "2018-12-27T11:11:01.129845Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "\n",
    "def animate(i) :\n",
    "    graph_data = emotion\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for emo in graph_data:\n",
    "        xs.append(emo[0])\n",
    "        ys.append(emo[1])\n",
    "    ax1.clear()\n",
    "    ax1.plot(xs,ys)\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, interval=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XIV. Enhanced Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This basic step is now woring properly and results are quite satisfying. There are lots of sources of improvements we'll try to implement over time :\n",
    "- add features from manually selected filters (e.g Gabor filters)\n",
    "- take into account the frequency of eye blinks\n",
    "- take into account the symetry of the keypoints on a face\n",
    "- display all the keypoints of the face\n",
    "- align the face by scaling of the facial features\n",
    "- add emojis translating the emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Frequency of eye blink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T11:33:47.819768Z",
     "start_time": "2019-03-05T11:33:47.804826Z"
    }
   },
   "outputs": [],
   "source": [
    "def eye_aspect_ratio(eye):\n",
    "\tA = distance.euclidean(eye[1], eye[5])\n",
    "\tB = distance.euclidean(eye[2], eye[4])\n",
    "\tC = distance.euclidean(eye[0], eye[3])\n",
    "\tear = (A + B) / (2.0 * C)\n",
    "\treturn ear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T11:33:50.108890Z",
     "start_time": "2019-03-05T11:33:48.606201Z"
    }
   },
   "outputs": [],
   "source": [
    "thresh = 0.25\n",
    "frame_check = 20\n",
    "face_detect = dlib.get_frontal_face_detector()\n",
    "predictor_landmarks = dlib.shape_predictor(\"/Users/maelfabien/Desktop/LocalDB/Videos/landmarks/shape_predictor_68_face_landmarks.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T11:33:50.116869Z",
     "start_time": "2019-03-05T11:33:50.112214Z"
    }
   },
   "outputs": [],
   "source": [
    "(lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
    "(rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Detect Keypoints to plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T11:33:51.024906Z",
     "start_time": "2019-03-05T11:33:51.019348Z"
    }
   },
   "outputs": [],
   "source": [
    "(nStart, nEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"nose\"]\n",
    "(mStart, mEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"mouth\"]\n",
    "(jStart, jEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"jaw\"]\n",
    "\n",
    "(eblStart, eblEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eyebrow\"]\n",
    "(ebrStart, ebrEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eyebrow\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Face Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T14:38:19.928738Z",
     "start_time": "2019-02-25T14:38:19.875145Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "desiredLeftEye=(0.35, 0.35)\n",
    "\n",
    "def align(gray, rect):\n",
    "    # convert the landmark (x, y)-coordinates to a NumPy array\n",
    "    shape = predictor(gray, rect)\n",
    "    shape = shape_to_np(shape)\n",
    " \n",
    "    # extract the left and right eye (x, y)-coordinates\n",
    "    (lStart, lEnd) = FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
    "    (rStart, rEnd) = FACIAL_LANDMARKS_IDXS[\"right_eye\"]\n",
    "    leftEyePts = shape[lStart:lEnd]\n",
    "    rightEyePts = shape[rStart:rEnd]\n",
    "        \n",
    "    # compute the center of mass for each eye\n",
    "    leftEyeCenter = leftEyePts.mean(axis=0).astype(\"int\")\n",
    "    rightEyeCenter = rightEyePts.mean(axis=0).astype(\"int\")\n",
    " \n",
    "    # compute the angle between the eye centroids\n",
    "    dY = rightEyeCenter[1] - leftEyeCenter[1]\n",
    "    dX = rightEyeCenter[0] - leftEyeCenter[0]\n",
    "    angle = np.degrees(np.arctan2(dY, dX)) - 180\n",
    "        \n",
    "    # compute the desired right eye x-coordinate based on the\n",
    "    # desired x-coordinate of the left eye\n",
    "    desiredRightEyeX = 1.0 - desiredLeftEye[0]\n",
    " \n",
    "    # determine the scale of the new resulting image by taking\n",
    "    # the ratio of the distance between eyes in the *current*\n",
    "    # image to the ratio of distance between eyes in the\n",
    "    # *desired* image\n",
    "    dist = np.sqrt((dX ** 2) + (dY ** 2))\n",
    "    desiredDist = (desiredRightEyeX - desiredLeftEye[0])\n",
    "    desiredDist *= self.desiredFaceWidth\n",
    "    scale = desiredDist / dist\n",
    "        \n",
    "    # compute center (x, y)-coordinates (i.e., the median point)\n",
    "    # between the two eyes in the input image\n",
    "    eyesCenter = ((leftEyeCenter[0] + rightEyeCenter[0]) // 2,\n",
    "            (leftEyeCenter[1] + rightEyeCenter[1]) // 2)\n",
    " \n",
    "    # grab the rotation matrix for rotating and scaling the face\n",
    "    M = cv2.getRotationMatrix2D(eyesCenter, angle, scale)\n",
    " \n",
    "    # update the translation component of the matrix\n",
    "    tX = self.desiredFaceWidth * 0.5\n",
    "    tY = self.desiredFaceHeight * self.desiredLeftEye[1]\n",
    "    M[0, 2] += (tX - eyesCenter[0])\n",
    "    M[1, 2] += (tY - eyesCenter[1])\n",
    "        \n",
    "    # apply the affine transformation\n",
    "    (w, h) = (self.desiredFaceWidth, self.desiredFaceHeight)\n",
    "    #output = cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_CUBIC)\n",
    "    output = cv2.warpAffine(gray, M, (w, h), flags=cv2.INTER_CUBIC)\n",
    "    # return the aligned face\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T22:17:19.614670Z",
     "start_time": "2019-02-12T14:17:07.862Z"
    }
   },
   "source": [
    "## d. Final Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T14:39:53.871508Z",
     "start_time": "2019-02-25T14:38:22.486835Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Lancer la capture video\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "flag = 0\n",
    "j = 1\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_capture.read()\n",
    "    \n",
    "    face_index = 0\n",
    "    \n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    rects = face_detect(gray, 1)\n",
    "    #gray, detected_faces, coord = detect_face(frame)\n",
    "    \n",
    "    for (i, rect) in enumerate(rects):\n",
    "        \n",
    "        shape = predictor_landmarks(gray, rect)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "    \n",
    "        (x, y, w, h) = face_utils.rect_to_bb(rect)\n",
    "        face = gray[y:y+h,x:x+w]\n",
    "        \n",
    "        #Zoom sur la face extraite\n",
    "        face = zoom(face, (shape_x / face.shape[0],shape_y / face.shape[1]))\n",
    "        #cast type float\n",
    "        face = face.astype(np.float32)\n",
    "        #scale\n",
    "        face /= float(face.max())\n",
    "        face = np.reshape(face.flatten(), (1, 48, 48, 1))\n",
    "        prediction = model.predict(face)\n",
    "        prediction_result = np.argmax(prediction)\n",
    "        \n",
    "        # Rectangle around the face\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    \n",
    "        cv2.putText(frame, \"Face #{}\".format(i + 1), (x - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    " \n",
    "        for (j, k) in shape:\n",
    "            cv2.circle(frame, (j, k), 1, (0, 0, 255), -1)\n",
    "\n",
    "         # 12. Add prediction probabilities\n",
    "        cv2.putText(frame, \"Emotional report : \",(40,120), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 0)\n",
    "        cv2.putText(frame, \"Angry : \" + str(round(prediction[0][0],3)),(40,140), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 0)\n",
    "        cv2.putText(frame, \"Disgust : \" + str(round(prediction[0][1],3)),(40,160), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 0)\n",
    "        cv2.putText(frame, \"Fear : \" + str(round(prediction[0][2],3)),(40,180), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 1)\n",
    "        cv2.putText(frame, \"Happy : \" + str(round(prediction[0][3],3)),(40,200), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 1)\n",
    "        cv2.putText(frame, \"Sad : \" + str(round(prediction[0][4],3)),(40,220), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 1)\n",
    "        cv2.putText(frame, \"Surprise : \" + str(round(prediction[0][5],3)),(40,240), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 1)\n",
    "        cv2.putText(frame, \"Neutral : \" + str(round(prediction[0][6],3)),(40,260), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 1)\n",
    "        \n",
    "        # draw extracted face in the top right corner\n",
    "        #frame[face_index * shape_x: (face_index + 1) * shape_x, -1 * shape_y - 1:-1, :] = cv2.cvtColor(face * 255, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "        # 13. Annotate main image with a label\n",
    "        if prediction_result == 0 :\n",
    "            cv2.putText(frame, \"Happy\",(x+w-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        elif prediction_result == 1 :\n",
    "            cv2.putText(frame, \"Happy\",(x+w-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        elif prediction_result == 2 :\n",
    "            cv2.putText(frame, \"Happy\",(x+w-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        elif prediction_result == 3 :\n",
    "            cv2.putText(frame, \"Happy\",(x+w-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        elif prediction_result == 4 :\n",
    "            cv2.putText(frame, \"Happy\",(x+w-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        elif prediction_result == 5 :\n",
    "            cv2.putText(frame, \"Happy\",(x+w-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        else :\n",
    "            cv2.putText(frame, \"Happy\",(x+w-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                   \n",
    "        \n",
    "        # 5. Eye Detection and Blink Count\n",
    "        leftEye = shape[lStart:lEnd]\n",
    "        rightEye = shape[rStart:rEnd]\n",
    "        \n",
    "        # Compute Eye Aspect Ratio\n",
    "        leftEAR = eye_aspect_ratio(leftEye)\n",
    "        rightEAR = eye_aspect_ratio(rightEye)\n",
    "        ear = (leftEAR + rightEAR) / 2.0\n",
    "            \n",
    "        # And plot its contours\n",
    "        leftEyeHull = cv2.convexHull(leftEye)\n",
    "        rightEyeHull = cv2.convexHull(rightEye)\n",
    "        cv2.drawContours(frame, [leftEyeHull], -1, (0, 255, 0), 1)\n",
    "        cv2.drawContours(frame, [rightEyeHull], -1, (0, 255, 0), 1)\n",
    "        \n",
    "        # Compute total blinks and frequency\n",
    "        if ear < thresh:\n",
    "            flag += 1\n",
    "            #cv2.putText(frame, \"Blink\", (10, 200), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 0)\n",
    "        \n",
    "        cv2.putText(frame, \"Total blinks : \" + str(flag), (40, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, 155, 1)\n",
    "        #cv2.putText(frame, \"Blink Frequency : \" + str(int(flag/j)), (40, 220), cv2.FONT_HERSHEY_SIMPLEX, 0.5, 155, 0)\n",
    "        \n",
    "        # 6. Detect Nose\n",
    "        nose = shape[nStart:nEnd]\n",
    "        noseHull = cv2.convexHull(nose)\n",
    "        cv2.drawContours(frame, [noseHull], -1, (0, 255, 0), 1)\n",
    "\n",
    "        # 7. Detect Mouth\n",
    "        mouth = shape[mStart:mEnd]\n",
    "        mouthHull = cv2.convexHull(mouth)\n",
    "        cv2.drawContours(frame, [mouthHull], -1, (0, 255, 0), 1)\n",
    "            \n",
    "        # 8. Detect Jaw\n",
    "        jaw = shape[jStart:jEnd]\n",
    "        jawHull = cv2.convexHull(jaw)\n",
    "        cv2.drawContours(frame, [jawHull], -1, (0, 255, 0), 1)\n",
    "            \n",
    "        # 9. Detect Eyebrows\n",
    "        ebr = shape[ebrStart:ebrEnd]\n",
    "        ebrHull = cv2.convexHull(ebr)\n",
    "        cv2.drawContours(frame, [ebrHull], -1, (0, 255, 0), 1)\n",
    "        ebl = shape[eblStart:eblEnd]\n",
    "        eblHull = cv2.convexHull(ebl)\n",
    "        cv2.drawContours(frame, [eblHull], -1, (0, 255, 0), 1)\n",
    "            \n",
    "    cv2.putText(frame,'Number of Faces : ' + str(i+1),(40, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, 155, 1)\n",
    "    j = j + 1\n",
    "    cv2.imshow('Video', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything is done, release the capture\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XV. Deeper CNN Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T12:53:53.848583Z",
     "start_time": "2019-03-05T12:53:53.841254Z"
    }
   },
   "source": [
    "## Inception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecture Inspired by : https://ieeexplore.ieee.org/document/7477450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T00:01:08.358049Z",
     "start_time": "2019-02-21T00:01:07.578116Z"
    }
   },
   "outputs": [],
   "source": [
    "input_img = Input(shape=(shape_x, shape_y, 1))\n",
    "\n",
    "layer1 = Conv2D(10, (3, 3), padding='same', activation='relu')(input_img)\n",
    "layer1_2 = Conv2D(20, (3, 3), padding='same', activation='relu')(layer1)\n",
    "layer2 = MaxPooling2D(pool_size=(3, 3))(layer1_2)\n",
    "bn1 = BatchNormalization()(layer2)\n",
    "\n",
    "layer3 = Conv2D(30, (3, 3), padding='same', activation='relu')(bn1)\n",
    "layer3_2 = Conv2D(40, (3, 3), padding='same', activation='relu')(layer3)\n",
    "layer4 = MaxPooling2D(pool_size=(3, 3))(layer3_2)\n",
    "bn2 = BatchNormalization()(layer4)\n",
    "\n",
    "layer5 = Conv2D(50, (3, 3), padding='same', activation='relu')(bn2)\n",
    "layer5_2 = Conv2D(60, (3, 3), padding='same', activation='relu')(layer5)\n",
    "layer6 = MaxPooling2D(pool_size=(3, 3))(layer5_2)\n",
    "bn3 = BatchNormalization()(layer6)\n",
    "\n",
    "Conv11 = Conv2D(1, (1, 1), padding='same', activation='relu')(bn3)\n",
    "Conv33 = Conv2D(4, (3, 3), padding='same', activation='relu')(bn3)\n",
    "Conv332 = Conv2D(1, (3, 3), padding='same', activation='relu')(Conv33)\n",
    "Conv55 = Conv2D(4, (5, 5), padding='same', activation='relu')(bn3)\n",
    "Conv552 = Conv2D(1, (3, 3), padding='same', activation='relu')(Conv55)\n",
    "#Pool33 = MaxPooling2D(pool_size=(3, 3))(bn3)\n",
    "#ConvPool1 = Conv2D(4, (1, 1), padding='same', activation='relu')(Pool33)\n",
    "\n",
    "intermediate1 = keras.layers.concatenate([Conv11, Conv332, Conv552], axis=1)\n",
    "\n",
    "Conv2_11 = Conv2D(1, (1, 1), padding='same', activation='relu')(intermediate1)\n",
    "Conv2_33 = Conv2D(4, (3, 3), padding='same', activation='relu')(intermediate1)\n",
    "Conv2_332 = Conv2D(1, (3, 3), padding='same', activation='relu')(Conv2_33)\n",
    "Conv2_55 = Conv2D(4, (5, 5), padding='same', activation='relu')(intermediate1)\n",
    "Conv2_552 = Conv2D(1, (3, 3), padding='same', activation='relu')(Conv2_55)\n",
    "#Pool2_33 = MaxPooling2D(pool_size=(3, 3))(intermediate1)\n",
    "#ConvPool2 = Conv2D(4, (1, 1), padding='same', activation='relu')(Pool2_33)\n",
    "\n",
    "intermediate2 = keras.layers.concatenate([Conv2_11, Conv2_332, Conv2_552], axis=1)\n",
    "\n",
    "Conv3_11 = Conv2D(1, (1, 1), padding='same', activation='relu')(intermediate2)\n",
    "Conv3_33 = Conv2D(4, (3, 3), padding='same', activation='relu')(intermediate2)\n",
    "Conv3_332 = Conv2D(1, (3, 3), padding='same', activation='relu')(Conv3_33)\n",
    "Conv3_55 = Conv2D(4, (5, 5), padding='same', activation='relu')(intermediate2)\n",
    "Conv3_552 = Conv2D(1, (3, 3), padding='same', activation='relu')(Conv3_55)\n",
    "#Pool3_33 = MaxPooling2D(pool_size=(3, 3))(intermediate2)\n",
    "#ConvPool3 = Conv2D(4, (1, 1), padding='same', activation='relu')(Pool3_33)\n",
    "\n",
    "intermediate3 = keras.layers.concatenate([Conv3_11, Conv3_332, Conv3_552], axis=1)\n",
    "\n",
    "#Pool4 = MaxPooling2D(pool_size=(3, 3))(intermediate3)\n",
    "\n",
    "Flat = Flatten()(intermediate3)\n",
    "\n",
    "Dense1 = Dense(25, activation='relu')(Flat)\n",
    "Dense2 = Dense(15, activation='relu')(Dense1)\n",
    "Dense3 = Dense(7, activation='softmax')(Dense2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T00:01:09.749535Z",
     "start_time": "2019-02-21T00:01:09.742468Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Model([input_img], Dense3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T00:01:12.233923Z",
     "start_time": "2019-02-21T00:01:11.954050Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_model(model, to_file='model_images/model_plot_3.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-19T17:20:41.098205Z",
     "start_time": "2019-02-19T17:20:41.090918Z"
    }
   },
   "source": [
    "<img src='model_images/model_plot_3.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T00:01:42.908439Z",
     "start_time": "2019-02-21T00:01:42.849978Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    " \n",
    "batch_size = 128\n",
    "epochs = 150\n",
    "\n",
    "# Fit the model on the batches generated by datagen.flow()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T06:23:06.441404Z",
     "start_time": "2019-02-21T00:01:45.480549Z"
    }
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        zoom_range=0.2,          # randomly zoom into images\n",
    "        rotation_range=10,       # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,   # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,    # randomly flip images\n",
    "        vertical_flip=False)     # randomly flip images\n",
    "\n",
    "history = model.fit_generator(\n",
    "    datagen.flow(train_data, train_labels_one_hot, batch_size=batch_size),\n",
    "    steps_per_epoch=int(np.ceil(train_data.shape[0] / float(batch_size))),\n",
    "    epochs = epochs, \n",
    "    validation_data=(test_data, test_labels_one_hot)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T06:54:44.637469Z",
     "start_time": "2019-02-20T06:54:43.957235Z"
    }
   },
   "outputs": [],
   "source": [
    "#Plotting accuracy and loss curves for 2nd model\n",
    "\n",
    "# Loss Curves\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(history.history['loss'],'r',linewidth=2.0)\n",
    "plt.plot(history.history['val_loss'],'b',linewidth=2.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.title('Loss Curves',fontsize=16)\n",
    " \n",
    "# Accuracy Curves\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(history.history['acc'],'r',linewidth=2.0)\n",
    "plt.plot(history.history['val_acc'],'b',linewidth=2.0)\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Accuracy',fontsize=16)\n",
    "plt.title('Accuracy Curves',fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-20T06:54:08.363906Z",
     "start_time": "2019-02-20T06:54:01.210003Z"
    }
   },
   "outputs": [],
   "source": [
    "#save the model weights\n",
    "json_string = model.to_json()\n",
    "model.save_weights(local_path + 'savedmodels/model_deep_2.h5')\n",
    "open(local_path + 'savedmodels/model_deep_2.json', 'w').write(json_string)\n",
    "#model.save_weights(local_path + 'savedmodels/Emotion_Face_Detection_Model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T14:36:36.365484Z",
     "start_time": "2019-02-25T14:36:36.018683Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(local_path + 'savedmodels/model_deep_2.json','r') as f:\n",
    "    json = f.read()\n",
    "model = model_from_json(json)\n",
    "\n",
    "model.load_weights(local_path + 'savedmodels/model_deep_2.h5')\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X-Ception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T13:27:56.622306Z",
     "start_time": "2019-03-05T13:27:56.590276Z"
    }
   },
   "outputs": [],
   "source": [
    "def entry_flow(inputs) :\n",
    "    \n",
    "    x = Conv2D(32, 3, strides = 2, padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = Conv2D(64,3,padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    previous_block_activation = x\n",
    "    \n",
    "    for size in [128, 256, 728] :\n",
    "    \n",
    "        x = Activation('relu')(x)\n",
    "        x = SeparableConv2D(size, 3, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "    \n",
    "        x = Activation('relu')(x)\n",
    "        x = SeparableConv2D(size, 3, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        \n",
    "        x = MaxPooling2D(3, strides=2, padding='same')(x)\n",
    "        \n",
    "        residual = Conv2D(size, 1, strides=2, padding='same')(previous_block_activation)\n",
    "        \n",
    "        x = keras.layers.Add()([x, residual])\n",
    "        previous_block_activation = x\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T13:28:57.162764Z",
     "start_time": "2019-03-05T13:28:57.150217Z"
    }
   },
   "outputs": [],
   "source": [
    "def middle_flow(x, num_blocks=8) :\n",
    "    \n",
    "    previous_block_activation = x\n",
    "    \n",
    "    for _ in range(num_blocks) :\n",
    "    \n",
    "        x = Activation('relu')(x)\n",
    "        x = SeparableConv2D(728, 3, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "    \n",
    "        x = Activation('relu')(x)\n",
    "        x = SeparableConv2D(728, 3, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        \n",
    "        x = Activation('relu')(x)\n",
    "        x = SeparableConv2D(728, 3, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        \n",
    "        x = keras.layers.Add()([x, previous_block_activation])\n",
    "        previous_block_activation = x\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T13:28:57.664258Z",
     "start_time": "2019-03-05T13:28:57.641837Z"
    }
   },
   "outputs": [],
   "source": [
    "def exit_flow(x, num_classes=7) :\n",
    "    \n",
    "    previous_block_activation = x\n",
    "    \n",
    "    x = Activation('relu')(x)\n",
    "    x = SeparableConv2D(728, 3, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Activation('relu')(x)\n",
    "    x = SeparableConv2D(1024, 3, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = MaxPooling2D(3, strides=2, padding='same')(x)\n",
    "    \n",
    "    residual = Conv2D(1024, 1, strides=2, padding='same')(previous_block_activation)\n",
    "    x = keras.layers.Add()([x, residual])\n",
    "      \n",
    "    x = Activation('relu')(x)\n",
    "    x = SeparableConv2D(728, 3, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Activation('relu')(x)\n",
    "    x = SeparableConv2D(1024, 3, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T13:30:39.609880Z",
     "start_time": "2019-03-05T13:30:35.235092Z"
    }
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(shape_x, shape_y, 1))\n",
    "outputs = exit_flow(middle_flow(entry_flow(inputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T13:31:03.538668Z",
     "start_time": "2019-03-05T13:31:03.527169Z"
    }
   },
   "outputs": [],
   "source": [
    "xception = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T13:31:26.829572Z",
     "start_time": "2019-03-05T13:31:26.089228Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_model(xception, to_file='model_images/model_plot_4.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='model_images/model_plot_4.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T13:33:55.727234Z",
     "start_time": "2019-03-05T13:33:55.696457Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xception.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-05T13:35:23.936135Z",
     "start_time": "2019-03-05T13:35:23.864143Z"
    }
   },
   "outputs": [],
   "source": [
    "xception.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    " \n",
    "batch_size = 128\n",
    "epochs = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T09:55:53.046158Z",
     "start_time": "2019-03-05T13:35:43.324131Z"
    }
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        zoom_range=0.2,          # randomly zoom into images\n",
    "        rotation_range=10,       # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,   # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,    # randomly flip images\n",
    "        vertical_flip=False)     # randomly flip images\n",
    "\n",
    "history = xception.fit_generator(\n",
    "    datagen.flow(train_data, train_labels_one_hot, batch_size=batch_size),\n",
    "    steps_per_epoch=int(np.ceil(train_data.shape[0] / float(batch_size))),\n",
    "    epochs = epochs, \n",
    "    validation_data=(test_data, test_labels_one_hot)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
